This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-13T13:33:08.548Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
cmd/main.go
docker-compose.yml
Dockerfile
go.mod
go.sum
internal/collector/collector.go
internal/config/config.go
internal/config/manager.go
internal/database/migration.go
internal/database/migrations/000001_initial_schema.down.sql
internal/database/migrations/000001_initial_schema.up.sql
internal/model/device.go
internal/protocol/protocol.go
internal/protocol/snmp/handler.go
internal/queue/queue.go
internal/queue/ratelimit.go
internal/queue/retry.go

================================================================
Repository Files
================================================================

================
File: cmd/main.go
================
package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"syscall"
	"time"

	"data_puller/internal/collector"
	"data_puller/internal/config"
	"data_puller/internal/model"
	"data_puller/internal/protocol/snmp"
)

func main() {
	// Create context with cancellation
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel() // Ensure cancellation on function exit

	// Initialize config manager with database connection string
	dsn := fmt.Sprintf("postgres://%s:%s@%s:%s/%s?sslmode=disable",
		os.Getenv("DB_USER"),
		os.Getenv("DB_PASSWORD"),
		os.Getenv("DB_HOST"),
		os.Getenv("DB_PORT"),
		os.Getenv("DB_NAME"),
	)

	configManager, err := config.NewConfigManager(dsn)
	if err != nil {
		log.Fatalf("Failed to create config manager: %v", err)
	}
	defer configManager.Close()

	if err := configManager.Initialize(ctx); err != nil {
		log.Fatalf("Failed to initialize config manager: %v", err)
	}

	// Create collector with specified queue size and number of workers
	collector := collector.NewCollector(1000, 5) // 1000 queue size, 5 workers

	// Register SNMP handler with the collector
	if err := collector.RegisterHandler("snmp", snmp.NewSNMPHandler()); err != nil {
		log.Fatalf("Failed to register SNMP handler: %v", err)
	}

	// Start the collector to begin processing
	if err := collector.Start(ctx); err != nil {
		log.Fatalf("Failed to start collector: %v", err)
	}

	// Load initial devices from the config manager
	devices, err := configManager.GetDevices(ctx)
	if err != nil {
		log.Fatalf("Failed to get devices: %v", err)
	}

	// Configure initial devices by retrieving their queries and configs
	for _, device := range devices {
		// Get device queries
		queries, err := configManager.GetDeviceQueries(ctx, device.ID)
		if err != nil {
			log.Printf("Failed to get queries for device %d: %v", device.ID, err)
			continue
		}

		// Get device configs
		configs, err := configManager.GetDeviceConfig(ctx, device.ID)
		if err != nil {
			log.Printf("Failed to get configs for device %d: %v", device.ID, err)
			continue
		}

		// Add device to collector
		if err := collector.AddDevice(device, configs); err != nil {
			log.Printf("Failed to add device %d: %v", device.ID, err)
			continue
		}

		// Schedule queries
		for _, query := range queries {
			if err := collector.ScheduleQuery(ctx, device.ID, query); err != nil {
				log.Printf("Failed to schedule query for device %d: %v", device.ID, err)
			}
		}
	}

	// Watch for configuration changes in a separate goroutine
	changes, err := configManager.WatchChanges(ctx)
	if err != nil {
		log.Fatalf("Failed to watch config changes: %v", err)
	}

	// Handle config changes
	go func() {
		for change := range changes {
			switch change.Type {
			case "device_added":
				device := change.Data.(*model.Device)
				configs, _ := configManager.GetDeviceConfig(ctx, device.ID)
				collector.AddDevice(device, configs)

			case "device_updated":
				device := change.Data.(*model.Device)
				configs, _ := configManager.GetDeviceConfig(ctx, device.ID)
				collector.UpdateDevice(device, configs)

			case "device_removed":
				collector.RemoveDevice(change.DeviceID)

			case "query_updated":
				query := change.Data.(*model.DeviceQuery)
				collector.ScheduleQuery(ctx, query.DeviceID, query)
			}
		}
	}()

	// Handle shutdown gracefully on receiving termination signals
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	<-sigChan
	log.Println("Shutting down...")

	// Create shutdown context with timeout
	shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer shutdownCancel()

	// Cancel main context to stop new work
	cancel()

	// Stop collector
	if err := collector.Stop(); err != nil {
		log.Printf("Error stopping collector: %v", err)
	}

	// Wait for shutdown context
	<-shutdownCtx.Done()
	log.Println("Shutdown complete")
}

================
File: docker-compose.yml
================
services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: inwatch
      POSTGRES_PASSWORD: inwatch123
      POSTGRES_DB: inwatch
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U inwatch"]
      interval: 10s
      timeout: 5s
      retries: 5

  data_puller:
    build: .
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_USER: inwatch
      DB_PASSWORD: inwatch123
      DB_NAME: inwatch
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  postgres_data:

================
File: Dockerfile
================
FROM golang:1.21-alpine AS builder

WORKDIR /app

# Install required system packages
RUN apk add --no-cache gcc musl-dev

# Copy go mod files
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY . .

# Build the application
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o data_puller ./cmd/main.go

# Final stage
FROM alpine:3.19

WORKDIR /app

# Copy binary from builder
COPY --from=builder /app/data_puller .
COPY --from=builder /app/migrations ./migrations

# Create non-root user
RUN adduser -D appuser
USER appuser

CMD ["./data_puller"]

================
File: go.mod
================
module data_puller

go 1.21

require (
	github.com/golang-migrate/migrate/v4 v4.17.0
	github.com/gosnmp/gosnmp v1.37.0
	github.com/lib/pq v1.10.9
)

require (
	github.com/hashicorp/errwrap v1.1.0 // indirect
	github.com/hashicorp/go-multierror v1.1.1 // indirect
	go.uber.org/atomic v1.11.0 // indirect
)

================
File: go.sum
================
github.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161 h1:L/gRVlceqvL25UVaW/CKtUDjefjrs0SPonmDGUVOYP0=
github.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161/go.mod h1:xomTg63KZ2rFqZQzSB4Vz2SUXa1BpHTVz9L5PTmPC4E=
github.com/Microsoft/go-winio v0.6.1 h1:9/kr64B9VUZrLm5YYwbGtUJnMgqWVOdUAXu6Migciow=
github.com/Microsoft/go-winio v0.6.1/go.mod h1:LRdKpFKfdobln8UmuiYcKPot9D2v6svN5+sAH+4kjUM=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/dhui/dktest v0.4.0 h1:z05UmuXZHO/bgj/ds2bGMBu8FI4WA+Ag/m3ghL+om7M=
github.com/dhui/dktest v0.4.0/go.mod h1:v/Dbz1LgCBOi2Uki2nUqLBGa83hWBGFMu5MrgMDCc78=
github.com/docker/distribution v2.8.2+incompatible h1:T3de5rq0dB1j30rp0sA2rER+m322EBzniBPB6ZIzuh8=
github.com/docker/distribution v2.8.2+incompatible/go.mod h1:J2gT2udsDAN96Uj4KfcMRqY0/ypR+oyYUYmja8H+y+w=
github.com/docker/docker v24.0.7+incompatible h1:Wo6l37AuwP3JaMnZa226lzVXGA3F9Ig1seQen0cKYlM=
github.com/docker/docker v24.0.7+incompatible/go.mod h1:eEKB0N0r5NX/I1kEveEz05bcu8tLC/8azJZsviup8Sk=
github.com/docker/go-connections v0.4.0 h1:El9xVISelRB7BuFusrZozjnkIM5YnzCViNKohAFqRJQ=
github.com/docker/go-connections v0.4.0/go.mod h1:Gbd7IOopHjR8Iph03tsViu4nIes5XhDvyHbTtUxmeec=
github.com/docker/go-units v0.5.0 h1:69rxXcBk27SvSaaxTtLh/8llcHD8vYHT7WSdRZ/jvr4=
github.com/docker/go-units v0.5.0/go.mod h1:fgPhTUdO+D/Jk86RDLlptpiXQzgHJF7gydDDbaIK4Dk=
github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
github.com/golang-migrate/migrate/v4 v4.17.0 h1:rd40H3QXU0AA4IoLllFcEAEo9dYKRHYND2gB4p7xcaU=
github.com/golang-migrate/migrate/v4 v4.17.0/go.mod h1:+Cp2mtLP4/aXDTKb9wmXYitdrNx2HGs45rbWAo6OsKM=
github.com/gosnmp/gosnmp v1.37.0 h1:/Tf8D3b9wrnNuf/SfbvO+44mPrjVphBhRtcGg22V07Y=
github.com/gosnmp/gosnmp v1.37.0/go.mod h1:GDH9vNqpsD7f2HvZhKs5dlqSEcAS6s6Qp099oZRCR+M=
github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=
github.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=
github.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=
github.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=
github.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=
github.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=
github.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
github.com/moby/term v0.5.0 h1:xt8Q1nalod/v7BqbG21f8mQPqH+xAaC9C3N3wfWbVP0=
github.com/moby/term v0.5.0/go.mod h1:8FzsFHVUBGZdbDsJw/ot+X+d5HLUbvklYLJ9uGfcI3Y=
github.com/morikuni/aec v1.0.0 h1:nP9CBfwrvYnBRgY6qfDQkygYDmYwOilePFkwzv4dU8A=
github.com/morikuni/aec v1.0.0/go.mod h1:BbKIizmSmc5MMPqRYbxO4ZU0S0+P200+tUnFx7PXmsc=
github.com/opencontainers/go-digest v1.0.0 h1:apOUWs51W5PlhuyGyz9FCeeBIOUDA/6nW8Oi/yOhh5U=
github.com/opencontainers/go-digest v1.0.0/go.mod h1:0JzlMkj0TRzQZfJkVvzbP0HBR3IKzErnv2BNG4W4MAM=
github.com/opencontainers/image-spec v1.0.2 h1:9yCKha/T5XdGtO0q9Q9a6T5NUCsTn/DrBg0D7ufOcFM=
github.com/opencontainers/image-spec v1.0.2/go.mod h1:BtxoFyWECRxE4U/7sNtV5W15zMzWCbyJoFRP3s7yZA0=
github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/stretchr/testify v1.8.4 h1:CcVxjf3Q8PM0mHUKJCdn+eZZtm5yQwehR5yeSVQQcUk=
github.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=
go.uber.org/atomic v1.11.0 h1:ZvwS0R+56ePWxUNi+Atn9dWONBPp/AUETXlHW0DxSjE=
go.uber.org/atomic v1.11.0/go.mod h1:LUxbIzbOniOlMKjJjyPfpl4v+PKK2cNJn91OQbhoJI0=
golang.org/x/mod v0.11.0 h1:bUO06HqtnRcc/7l71XBe4WcqTZ+3AH1J59zWDDwLKgU=
golang.org/x/mod v0.11.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=
golang.org/x/net v0.18.0 h1:mIYleuAkSbHh0tCv7RvjL3F6ZVbLjq4+R7zbOn3Kokg=
golang.org/x/net v0.18.0/go.mod h1:/czyP5RqHAH4odGYxBJ1qz0+CE5WZ+2j1YgoEo8F2jQ=
golang.org/x/sys v0.15.0 h1:h48lPFYpsTvQJZF4EKyI4aLHaev3CxivZmv7yZig9pc=
golang.org/x/sys v0.15.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/tools v0.10.0 h1:tvDr/iQoUqNdohiYm0LmmKcBk+q86lb9EprIUFhHHGg=
golang.org/x/tools v0.10.0/go.mod h1:UJwyiVBsOA2uwvK/e5OY3GTpDUJriEd+/YlqAwLPmyM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=

================
File: internal/collector/collector.go
================
// internal/collector/collector.go
package collector

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"

	"data_puller/internal/model"
	"data_puller/internal/protocol"
	"data_puller/internal/queue"
)

// Collector manages devices and their data collection.
type Collector struct {
	mu          sync.RWMutex
	devices     map[int64]*model.Device
	handlers    map[string]protocol.Handler
	queue       *queue.WorkQueue
	rateLimiter *queue.RateLimiter
	retryPolicy queue.RetryPolicy
	configs     map[int64]map[string]string // deviceID -> config map
}

// NewCollector creates a new Collector instance with specified queue size and worker count.
func NewCollector(queueSize, workerCount int) *Collector {
	return &Collector{
		devices:     make(map[int64]*model.Device),
		handlers:    make(map[string]protocol.Handler),
		queue:       queue.NewWorkQueue(workerCount, queueSize),
		rateLimiter: queue.NewRateLimiter(time.Second),
		retryPolicy: queue.DefaultRetryPolicy(),
		configs:     make(map[int64]map[string]string),
	}
}

// RegisterHandler registers a new protocol handler for the collector.
func (c *Collector) RegisterHandler(protocol string, handler protocol.Handler) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	if _, exists := c.handlers[protocol]; exists {
		return fmt.Errorf("handler for protocol %s already registered", protocol)
	}

	c.handlers[protocol] = handler
	return nil
}

// Start initializes handlers and starts the work queue.
func (c *Collector) Start(ctx context.Context) error {
	// Initialize handlers
	for _, handler := range c.handlers {
		if err := handler.Initialize(ctx); err != nil {
			return fmt.Errorf("failed to initialize handler: %w", err)
		}
	}

	// Start work queue
	if err := c.queue.Start(ctx); err != nil {
		return fmt.Errorf("failed to start work queue: %w", err)
	}

	// Setup work processor
	go c.processWork(ctx)

	return nil
}

// processWork continuously processes work items from the queue.
func (c *Collector) processWork(ctx context.Context) {
	for work := range c.queue.WorkChan() {
		go c.handleWork(ctx, work)
	}
}

// handleWork processes a single work item, applying rate limiting and executing the handler.
func (c *Collector) handleWork(ctx context.Context, work *queue.Work) {
	// Rate limiting
	if err := c.rateLimiter.Wait(ctx, work.Device.ID); err != nil {
		log.Printf("Rate limiter error for device %d: %v", work.Device.ID, err)
		return
	}

	// Get handler for protocol
	c.mu.RLock()
	handler, exists := c.handlers[work.Device.Protocol]
	deviceConfig := c.configs[work.Device.ID]
	c.mu.RUnlock()

	if !exists {
		log.Printf("No handler found for protocol %s", work.Device.Protocol)
		return
	}

	// Execute work with retry policy
	err := c.retryPolicy.DoWithRetry(ctx, func() error {
		result, err := handler.Poll(ctx, work.Device, work.Query)
		if err != nil {
			return fmt.Errorf("poll failed: %w", err)
		}

		// Process result
		c.processResult(result)
		return nil
	})

	if err != nil {
		log.Printf("Failed to collect data from device %d (%s): %v",
			work.Device.ID, work.Device.Name, err)

		// Update device status if needed
		c.updateDeviceStatus(work.Device.ID, "error")
	}
}

// processResult handles the result of a query, logging any errors.
func (c *Collector) processResult(result *protocol.QueryResult) {
	if result.Error != nil {
		log.Printf("Error in query result for device %d, parameter %s: %v",
			result.DeviceID, result.ParameterName, result.Error)
		return
	}

	// Currently just logging the result
	log.Printf("Data collected - Device: %d, Parameter: %s, Value: %v, Timestamp: %d",
		result.DeviceID, result.ParameterName, result.Value, result.Timestamp)
}

// AddDevice adds a new device to the collector with its configurations.
func (c *Collector) AddDevice(device *model.Device, configs map[string]string) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	// Validate device protocol
	if _, exists := c.handlers[device.Protocol]; !exists {
		return fmt.Errorf("unsupported protocol: %s", device.Protocol)
	}

	c.devices[device.ID] = device
	c.configs[device.ID] = configs

	return nil
}

// RemoveDevice removes a device from the collector and cancels its scheduled work.
func (c *Collector) RemoveDevice(deviceID int64) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	delete(c.devices, deviceID)
	delete(c.configs, deviceID)

	// Cancel all scheduled work for this device
	return c.queue.CancelDevice(deviceID)
}

// UpdateDevice updates an existing device's information and configurations.
func (c *Collector) UpdateDevice(device *model.Device, configs map[string]string) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	// Cancel existing schedules
	c.queue.CancelDevice(device.ID)

	// Update device and configs
	c.devices[device.ID] = device
	c.configs[device.ID] = configs

	return nil
}

// updateDeviceStatus updates the status of a device.
func (c *Collector) updateDeviceStatus(deviceID int64, status string) {
	c.mu.Lock()
	defer c.mu.Unlock()

	if device, exists := c.devices[deviceID]; exists {
		device.Status = status
	}
}

// ScheduleQuery schedules a query for a specific device.
func (c *Collector) ScheduleQuery(ctx context.Context, deviceID int64, query *model.DeviceQuery) error {
	c.mu.RLock()
	device, exists := c.devices[deviceID]
	c.mu.RUnlock()

	if !exists {
		return fmt.Errorf("device not found: %d", deviceID)
	}

	work := &queue.Work{
		Device: device,
		Query:  query,
	}

	interval := time.Duration(query.PollInterval) * time.Second
	return c.queue.Schedule(ctx, work, interval)
}

// Stop stops all handlers and cleans up resources.
func (c *Collector) Stop() error {
	c.mu.Lock()
	defer c.mu.Unlock()

	// Stop all handlers
	for _, handler := range c.handlers {
		if err := handler.Close(); err != nil {
			log.Printf("Error closing handler: %v", err)
		}
	}

	return nil
}

================
File: internal/config/config.go
================
package config

import (
	"context"
	"data_puller/internal/model"
)

// ConfigManager defines the methods for managing configuration changes.
type ConfigManager interface {
	// Initialize sets up the configuration manager.
	Initialize(ctx context.Context) error

	// WatchChanges listens for configuration changes and returns a channel to receive them.
	WatchChanges(ctx context.Context) (<-chan ConfigChange, error)

	// GetDevices retrieves a list of devices.
	GetDevices(ctx context.Context) ([]*model.Device, error)

	// GetDeviceQueries retrieves queries associated with a specific device.
	GetDeviceQueries(ctx context.Context, deviceID int64) ([]*model.DeviceQuery, error)
}

// ConfigChange represents a change in configuration.
type ConfigChange struct {
	Type     string      // Type of change: "device_added", "device_updated", "device_removed", "query_updated"
	DeviceID int64       // ID of the device associated with the change
	Data     interface{} // Additional data related to the change
}

================
File: internal/config/manager.go
================
package config

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"log"
	"sync"
	"time"

	"data_puller/internal/model"

	"github.com/lib/pq"
)

// dbConfigManager implements dbConfigManager interface.
type dbConfigManager struct {
	db       *sql.DB           // Database connection
	listener *pq.Listener      // PostgreSQL listener for notifications
	changes  chan ConfigChange // Channel to send configuration changes
	mu       sync.RWMutex      // Mutex for concurrent access
}

// NewConfigManager initializes a new dbConfigManager with a database connection.
func NewConfigManager(dsn string) (*dbConfigManager, error) {
	db, err := sql.Open("postgres", dsn)
	if err != nil {
		return nil, fmt.Errorf("failed to open database: %w", err)
	}

	// Ping to verify connection
	if err := db.Ping(); err != nil {
		db.Close()
		return nil, fmt.Errorf("failed to ping database: %w", err)
	}

	// Create a new PostgreSQL listener for notifications
	listener := pq.NewListener(dsn,
		10*time.Second, // Minimum reconnection interval
		1*time.Minute,  // Maximum reconnection interval
		func(ev pq.ListenerEventType, err error) {
			if err != nil {
				log.Printf("Listen error: %v\n", err)
			}
		})

	return &dbConfigManager{
		db:       db,
		listener: listener,
		changes:  make(chan ConfigChange, 100), // Buffered channel for changes
	}, nil
}

// Initialize starts listening for configuration changes.
func (m *dbConfigManager) Initialize(ctx context.Context) error {
	if err := m.listener.Listen("config_changes"); err != nil {
		return fmt.Errorf("failed to start listening: %w", err)
	}

	// Start processing notifications in background
	go m.processNotifications(ctx)

	return nil
}

// dbNotification represents the structure of a database notification.
type dbNotification struct {
	Table    string         `json:"table"`     // Table name where the change occurred
	Action   string         `json:"action"`    // Action type (INSERT, UPDATE, DELETE)
	DeviceID int64          `json:"device_id"` // ID of the device affected
	Data     map[string]any `json:"data"`      // Data associated with the change
}

// processNotifications listens for and processes database notifications.
func (m *dbConfigManager) processNotifications(ctx context.Context) {
	for {
		select {
		case <-ctx.Done():
			return // Exit if context is done

		case n := <-m.listener.Notify:
			if n == nil {
				continue // Skip if notification is nil
			}

			var notification dbNotification
			if err := json.Unmarshal([]byte(n.Extra), &notification); err != nil {
				log.Printf("Failed to unmarshal notification: %v", err)
				continue
			}

			change := ConfigChange{
				DeviceID: notification.DeviceID, // Set the device ID for the change
			}

			// Determine the type of change based on the notification
			switch notification.Table {
			case "devices":
				switch notification.Action {
				case "INSERT":
					change.Type = "device_added"
				case "UPDATE":
					change.Type = "device_updated"
				case "DELETE":
					change.Type = "device_removed"
				}

				if notification.Action != "DELETE" {
					device := &model.Device{}
					if err := mapToStruct(notification.Data, device); err != nil {
						log.Printf("Failed to convert device data: %v", err)
						continue
					}
					change.Data = device // Set the device data for the change
				}

			case "device_queries":
				change.Type = "query_updated"
				if notification.Action != "DELETE" {
					query := &model.DeviceQuery{}
					if err := mapToStruct(notification.Data, query); err != nil {
						log.Printf("Failed to convert query data: %v", err)
						continue
					}
					change.Data = query // Set the query data for the change
				}

			case "device_configs":
				change.Type = "config_updated"
				if notification.Action != "DELETE" {
					config := &model.DeviceConfig{}
					if err := mapToStruct(notification.Data, config); err != nil {
						log.Printf("Failed to convert config data: %v", err)
						continue
					}
					change.Data = config // Set the config data for the change
				}
			}

			// Send to changes channel, non-blocking
			select {
			case m.changes <- change:
			default:
				log.Printf("Changes channel full, dropped notification for device %d", notification.DeviceID)
			}
		}
	}
}

// GetDevices retrieves all active devices from the database.
func (m *dbConfigManager) GetDevices(ctx context.Context) ([]*model.Device, error) {
	query := `
        SELECT id, name, ip, port, protocol, status
        FROM devices
        WHERE status = 'active'
    `

	rows, err := m.db.QueryContext(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("failed to query devices: %w", err)
	}
	defer rows.Close()

	var devices []*model.Device
	for rows.Next() {
		device := &model.Device{}
		err := rows.Scan(
			&device.ID,
			&device.Name,
			&device.IP,
			&device.Port,
			&device.Protocol,
			&device.Status,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan device: %w", err)
		}
		devices = append(devices, device) // Append the device to the list
	}

	return devices, nil // Return the list of devices
}

// GetDeviceQueries retrieves all enabled queries for a specific device.
func (m *dbConfigManager) GetDeviceQueries(ctx context.Context, deviceID int64) ([]*model.DeviceQuery, error) {
	query := `
        SELECT id, device_id, query_string, parameter_name, data_type, poll_interval, enabled
        FROM device_queries
        WHERE device_id = $1 AND enabled = true
    `

	rows, err := m.db.QueryContext(ctx, query, deviceID)
	if err != nil {
		return nil, fmt.Errorf("failed to query device queries: %w", err)
	}
	defer rows.Close()

	var queries []*model.DeviceQuery
	for rows.Next() {
		q := &model.DeviceQuery{}
		err := rows.Scan(
			&q.ID,
			&q.DeviceID,
			&q.QueryString,
			&q.ParameterName,
			&q.DataType,
			&q.PollInterval,
			&q.Enabled,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to scan query: %w", err)
		}
		queries = append(queries, q) // Append the query to the list
	}

	return queries, nil // Return the list of queries
}

// GetDeviceConfig retrieves the configuration for a specific device.
func (m *dbConfigManager) GetDeviceConfig(ctx context.Context, deviceID int64) (map[string]string, error) {
	query := `
        SELECT config_key, config_value
        FROM device_configs
        WHERE device_id = $1
    `

	rows, err := m.db.QueryContext(ctx, query, deviceID)
	if err != nil {
		return nil, fmt.Errorf("failed to query device configs: %w", err)
	}
	defer rows.Close()

	configs := make(map[string]string) // Map to hold configuration key-value pairs
	for rows.Next() {
		var key, value string
		if err := rows.Scan(&key, &value); err != nil {
			return nil, fmt.Errorf("failed to scan config: %w", err)
		}
		configs[key] = value // Store the key-value pair in the map
	}

	return configs, nil // Return the configuration map
}

// WatchChanges returns a channel to receive configuration change notifications.
func (m *dbConfigManager) WatchChanges(ctx context.Context) (<-chan ConfigChange, error) {
	return m.changes, nil // Return the changes channel
}

// Close cleans up resources and closes the database connection.
func (m *dbConfigManager) Close() error {
	if err := m.listener.Close(); err != nil {
		log.Printf("Error closing listener: %v", err)
	}

	close(m.changes)    // Close the changes channel
	return m.db.Close() // Close the database connection
}

// Helper function to convert map to struct
func mapToStruct(data map[string]any, dest interface{}) error {
	jsonData, err := json.Marshal(data) // Marshal the map to JSON
	if err != nil {
		return err
	}
	return json.Unmarshal(jsonData, dest) // Unmarshal JSON to the destination struct
}

================
File: internal/database/migration.go
================
package database

import (
	"errors"
	"fmt"
	"log"

	"github.com/golang-migrate/migrate/v4"
	_ "github.com/golang-migrate/migrate/v4/database/postgres"
	_ "github.com/golang-migrate/migrate/v4/source/file"
)

// MigrationManager manages database migrations
type MigrationManager struct {
	dsn string // Data Source Name for the database connection
}

// NewMigrationManager creates a new MigrationManager with the provided database connection parameters
func NewMigrationManager(host, port, user, password, dbname string) *MigrationManager {
	// Construct the DSN for connecting to the PostgreSQL database
	dsn := fmt.Sprintf("postgres://%s:%s@%s:%s/%s?sslmode=disable",
		user, password, host, port, dbname)
	return &MigrationManager{dsn: dsn} // Return a new instance of MigrationManager
}

// Run applies all available migrations to the database
func (m *MigrationManager) Run() error {
	// Create a new migration instance
	migration, err := migrate.New(
		"file://migrations", // Path to migration files
		m.dsn,               // Database connection string
	)
	if err != nil {
		return fmt.Errorf("failed to create migration instance: %w", err) // Handle error
	}
	defer migration.Close() // Ensure the migration instance is closed after use

	// Apply the migrations
	if err := migration.Up(); err != nil {
		if errors.Is(err, migrate.ErrNoChange) {
			log.Println("No migration changes needed") // Log if no changes are needed
			return nil
		}
		return fmt.Errorf("failed to run migrations: %w", err) // Handle error
	}

	return nil // Return nil if migrations were successful
}

// Rollback reverts the last applied migration
func (m *MigrationManager) Rollback() error {
	// Create a new migration instance
	migration, err := migrate.New(
		"file://migrations", // Path to migration files
		m.dsn,               // Database connection string
	)
	if err != nil {
		return fmt.Errorf("failed to create migration instance: %w", err) // Handle error
	}
	defer migration.Close() // Ensure the migration instance is closed after use

	// Revert the last migration
	if err := migration.Down(); err != nil {
		if errors.Is(err, migrate.ErrNoChange) {
			log.Println("No rollback needed") // Log if no rollback is needed
			return nil
		}
		return fmt.Errorf("failed to rollback migrations: %w", err) // Handle error
	}

	return nil // Return nil if rollback was successful
}

================
File: internal/database/migrations/000001_initial_schema.down.sql
================
DROP TRIGGER IF EXISTS notify_config_changes ON device_configs;
DROP TRIGGER IF EXISTS notify_query_changes ON device_queries;
DROP TRIGGER IF EXISTS notify_device_changes ON devices;
DROP FUNCTION IF EXISTS notify_config_change();

DROP TRIGGER IF EXISTS update_device_configs_updated_at ON device_configs;
DROP TRIGGER IF EXISTS update_device_queries_updated_at ON device_queries;
DROP TRIGGER IF EXISTS update_devices_updated_at ON devices;
DROP FUNCTION IF EXISTS update_updated_at_column();

DROP TABLE IF EXISTS device_configs;
DROP TABLE IF EXISTS device_queries;
DROP TABLE IF EXISTS devices;

DROP TYPE IF EXISTS data_type;
DROP TYPE IF EXISTS device_status;
DROP TYPE IF EXISTS protocol_type;

================
File: internal/database/migrations/000001_initial_schema.up.sql
================
CREATE TYPE protocol_type AS ENUM ('snmp', 'tcp', 'modbus', 'rest');
CREATE TYPE device_status AS ENUM ('active', 'inactive', 'error');
CREATE TYPE data_type AS ENUM ('string', 'int', 'float', 'boolean');

CREATE TABLE devices (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    ip VARCHAR(255) NOT NULL,
    port INTEGER NOT NULL,
    protocol protocol_type NOT NULL,
    status device_status NOT NULL DEFAULT 'inactive',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (ip, port, protocol)
);

CREATE TABLE device_queries (
    id SERIAL PRIMARY KEY,
    device_id INTEGER NOT NULL REFERENCES devices(id) ON DELETE CASCADE,
    query_string VARCHAR(255) NOT NULL,
    parameter_name VARCHAR(255) NOT NULL,
    data_type data_type NOT NULL,
    poll_interval INTEGER NOT NULL DEFAULT 60,
    enabled BOOLEAN NOT NULL DEFAULT true,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (device_id, parameter_name)
);

CREATE TABLE device_configs (
    id SERIAL PRIMARY KEY,
    device_id INTEGER NOT NULL REFERENCES devices(id) ON DELETE CASCADE,
    config_key VARCHAR(255) NOT NULL,
    config_value TEXT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (device_id, config_key)
);

-- Trigger for updating updated_at
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_devices_updated_at
    BEFORE UPDATE ON devices
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_device_queries_updated_at
    BEFORE UPDATE ON device_queries
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_device_configs_updated_at
    BEFORE UPDATE ON device_configs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Notify function for configuration changes
CREATE OR REPLACE FUNCTION notify_config_change()
RETURNS TRIGGER AS $$
DECLARE
    payload TEXT;
BEGIN
    CASE TG_TABLE_NAME
    WHEN 'devices' THEN
        payload = json_build_object(
            'table', TG_TABLE_NAME,
            'action', TG_OP,
            'device_id', COALESCE(NEW.id, OLD.id),
            'data', row_to_json(NEW)
        )::text;
    WHEN 'device_queries' THEN
        payload = json_build_object(
            'table', TG_TABLE_NAME,
            'action', TG_OP,
            'device_id', COALESCE(NEW.device_id, OLD.device_id),
            'data', row_to_json(NEW)
        )::text;
    WHEN 'device_configs' THEN
        payload = json_build_object(
            'table', TG_TABLE_NAME,
            'action', TG_OP,
            'device_id', COALESCE(NEW.device_id, OLD.device_id),
            'data', row_to_json(NEW)
        )::text;
    END CASE;

    PERFORM pg_notify('config_changes', payload);
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER notify_device_changes
    AFTER INSERT OR UPDATE OR DELETE ON devices
    FOR EACH ROW EXECUTE FUNCTION notify_config_change();

CREATE TRIGGER notify_query_changes
    AFTER INSERT OR UPDATE OR DELETE ON device_queries
    FOR EACH ROW EXECUTE FUNCTION notify_config_change();

CREATE TRIGGER notify_config_changes
    AFTER INSERT OR UPDATE OR DELETE ON device_configs
    FOR EACH ROW EXECUTE FUNCTION notify_config_change();

================
File: internal/model/device.go
================
package model

// Device represents a network device with its attributes.
type Device struct {
	ID       int64  `json:"id"`       // Unique identifier for the device
	Name     string `json:"name"`     // Name of the device
	IP       string `json:"ip"`       // IP address of the device
	Port     int    `json:"port"`     // Port number for the device
	Protocol string `json:"protocol"` // Communication protocol used by the device
	Status   string `json:"status"`   // Current status of the device
}

// DeviceQuery represents a query related to a specific device.
type DeviceQuery struct {
	ID            int64  `json:"id"`             // Unique identifier for the query
	DeviceID      int64  `json:"device_id"`      // Identifier of the associated device
	QueryString   string `json:"query_string"`   // The actual query string
	ParameterName string `json:"parameter_name"` // Name of the parameter being queried
	DataType      string `json:"data_type"`      // Type of data expected from the query
	PollInterval  int    `json:"poll_interval"`  // Interval for polling the device
	Enabled       bool   `json:"enabled"`        // Indicates if the query is enabled
}

// DeviceConfig represents the configuration settings for a device.
type DeviceConfig struct {
	ID          int64  `json:"id"`           // Unique identifier for the configuration
	DeviceID    int64  `json:"device_id"`    // Identifier of the associated device
	ConfigKey   string `json:"config_key"`   // Key for the configuration setting
	ConfigValue string `json:"config_value"` // Value for the configuration setting
}

================
File: internal/protocol/protocol.go
================
package protocol

import (
	"context"
	"data_puller/internal/model"
)

// QueryResult represents the result of a query to a device.
type QueryResult struct {
	DeviceID      int64       // Unique identifier for the device
	ParameterName string      // Name of the parameter being queried
	Value         interface{} // Value returned from the query
	Timestamp     int64       // Time when the query was executed
	Error         error       // Error encountered during the query, if any
}

// Handler defines the interface for handling device queries.
type Handler interface {
	Initialize(ctx context.Context) error                                                           // Initializes the handler with the given context
	Poll(ctx context.Context, device *model.Device, query *model.DeviceQuery) (*QueryResult, error) // Polls the device for the query result
	Validate(device *model.Device, query *model.DeviceQuery) error                                  // Validates the device and query parameters
	Close() error                                                                                   // Closes the handler and releases any resources
}

================
File: internal/protocol/snmp/handler.go
================
package snmp

import (
	"context"
	"fmt"
	"strconv"
	"strings"
	"sync"
	"time"

	"data_puller/internal/model"
	"data_puller/internal/protocol"

	"github.com/gosnmp/gosnmp"
)

// SNMPHandler manages SNMP clients and their operations
type SNMPHandler struct {
	clients map[int64]*gosnmp.GoSNMP // Map of SNMP clients indexed by device ID
	mu      sync.RWMutex             // Mutex for concurrent access to clients
}

// NewSNMPHandler creates a new instance of SNMPHandler
func NewSNMPHandler() *SNMPHandler {
	return &SNMPHandler{
		clients: make(map[int64]*gosnmp.GoSNMP), // Initialize the clients map
	}
}

// Initialize prepares the SNMPHandler for use, if needed
func (h *SNMPHandler) Initialize(ctx context.Context) error {
	// Initialize handler-wide resources if needed
	return nil
}

// getClient retrieves or creates an SNMP client for the specified device
func (h *SNMPHandler) getClient(device *model.Device) (*gosnmp.GoSNMP, error) {
	h.mu.RLock() // Acquire read lock
	client, exists := h.clients[device.ID]
	h.mu.RUnlock() // Release read lock

	if exists {
		return client, nil // Return existing client if found
	}

	h.mu.Lock()         // Acquire write lock
	defer h.mu.Unlock() // Ensure write lock is released

	// Double check after acquiring write lock
	if client, exists := h.clients[device.ID]; exists {
		return client, nil // Return existing client if found
	}

	// Create a new SNMP client
	client = &gosnmp.GoSNMP{
		Target:    device.IP,
		Port:      uint16(device.Port),
		Community: "public", // This value should be retrieved from device_configs table
		Version:   gosnmp.Version2c,
		Timeout:   time.Duration(2) * time.Second,
		Retries:   3,
	}

	// Attempt to connect to the SNMP device
	if err := client.Connect(); err != nil {
		return nil, fmt.Errorf("failed to connect to SNMP device: %w", err)
	}

	h.clients[device.ID] = client // Store the new client
	return client, nil
}

// Poll retrieves data from the SNMP device based on the query
func (h *SNMPHandler) Poll(ctx context.Context, device *model.Device, query *model.DeviceQuery) (*protocol.QueryResult, error) {
	client, err := h.getClient(device) // Get the SNMP client
	if err != nil {
		return nil, err
	}

	// Prepare the result structure
	result := &protocol.QueryResult{
		DeviceID:      device.ID,
		ParameterName: query.ParameterName,
		Timestamp:     time.Now().Unix(),
	}

	oids := []string{query.QueryString} // OID to query
	packets, err := client.Get(oids)    // Perform the SNMP GET operation
	if err != nil {
		result.Error = err
		return result, err
	}

	if len(packets.Variables) == 0 {
		result.Error = fmt.Errorf("no data received")
		return result, result.Error
	}

	// Convert the received value based on DataType
	variable := packets.Variables[0]
	value, err := convertSNMPValue(variable, query.DataType) // Convert SNMP value
	if err != nil {
		result.Error = err
		return result, err
	}

	result.Value = value // Store the converted value
	return result, nil
}

// Validate checks if the device and query are valid for SNMP
func (h *SNMPHandler) Validate(device *model.Device, query *model.DeviceQuery) error {
	if device.Protocol != "snmp" {
		return fmt.Errorf("invalid protocol: expected snmp, got %s", device.Protocol)
	}

	// Check OID format
	if !isValidOID(query.QueryString) {
		return fmt.Errorf("invalid OID format: %s", query.QueryString)
	}

	return nil
}

// Close releases resources held by the SNMPHandler
func (h *SNMPHandler) Close() error {
	h.mu.Lock()         // Acquire write lock
	defer h.mu.Unlock() // Ensure write lock is released

	for _, client := range h.clients {
		client.Conn.Close() // Close each SNMP client connection
	}
	h.clients = make(map[int64]*gosnmp.GoSNMP) // Clear the clients map
	return nil
}

// convertSNMPValue converts the SNMP variable to the specified data type
func convertSNMPValue(variable gosnmp.SnmpPDU, dataType string) (interface{}, error) {
	switch dataType {
	case "string":
		return variable.Value.(string), nil // Convert to string
	case "int":
		switch v := variable.Value.(type) {
		case int:
			return v, nil // Convert to int
		case int32:
			return int(v), nil // Convert to int32
		case int64:
			return v, nil // Convert to int64
		default:
			return nil, fmt.Errorf("unexpected type for int conversion: %T", v)
		}
	case "float":
		switch v := variable.Value.(type) {
		case float32:
			return float64(v), nil // Convert to float32
		case float64:
			return v, nil // Convert to float64
		default:
			return nil, fmt.Errorf("unexpected type for float conversion: %T", v)
		}
	default:
		return nil, fmt.Errorf("unsupported data type: %s", dataType) // Handle unsupported data types
	}
}

// isValidOID checks if the provided OID is valid
func isValidOID(oid string) bool {
	// Simple OID validation
	// Example: .1.3.6.1.2.1.1.1.0
	if len(oid) == 0 || (oid[0] != '.' && oid[0] != '1') {
		return false
	}

	parts := strings.Split(strings.TrimPrefix(oid, "."), ".") // Split OID into parts
	for _, part := range parts {
		if _, err := strconv.Atoi(part); err != nil {
			return false // Return false if any part is not an integer
		}
	}
	return true // OID is valid
}

================
File: internal/queue/queue.go
================
package queue

import (
	"context"
	"fmt"
	"sync"
	"time"

	"data_puller/internal/model"
)

// Work represents a unit of work to be processed.
type Work struct {
	Device *model.Device
	Query  *model.DeviceQuery
}

// ScheduledWork represents work that is scheduled to run at a specific interval.
type ScheduledWork struct {
	Work     *Work
	Interval time.Duration
	NextRun  time.Time
	StopChan chan struct{}
}

// WorkQueue manages a queue of work items and their scheduling.
type WorkQueue struct {
	mu           sync.RWMutex
	workChan     chan *Work
	scheduled    map[string]*ScheduledWork // key: deviceID_parameterName
	workerCount  int
	maxQueueSize int
}

// NewWorkQueue initializes a new WorkQueue with the specified worker count and max queue size.
func NewWorkQueue(workerCount, maxQueueSize int) *WorkQueue {
	return &WorkQueue{
		workChan:     make(chan *Work, maxQueueSize),
		scheduled:    make(map[string]*ScheduledWork),
		workerCount:  workerCount,
		maxQueueSize: maxQueueSize,
	}
}

// Start begins the scheduler and worker goroutines.
func (q *WorkQueue) Start(ctx context.Context) error {
	// Start scheduler
	go q.scheduler(ctx)

	// Start workers
	for i := 0; i < q.workerCount; i++ {
		go q.worker(ctx, i)
	}

	return nil
}

// worker processes work items from the work channel.
func (q *WorkQueue) worker(ctx context.Context, id int) {
	for {
		select {
		case <-ctx.Done():
			return
		case work := <-q.workChan:
			// İşi gerçekleştirecek handler'a gönder
			// Bu kısım Collector tarafından implement edilecek
		}
	}
}

// WorkChan returns the channel for receiving work items.
func (q *WorkQueue) WorkChan() <-chan *Work {
	return q.workChan
}

// scheduler checks for scheduled work and enqueues it for processing.
func (q *WorkQueue) scheduler(ctx context.Context) {
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			q.checkScheduledWork(ctx)
		}
	}
}

// checkScheduledWork checks if any scheduled work is due to run.
func (q *WorkQueue) checkScheduledWork(ctx context.Context) {
	q.mu.RLock()
	defer q.mu.RUnlock()

	now := time.Now()
	for _, sw := range q.scheduled {
		if now.After(sw.NextRun) {
			select {
			case <-ctx.Done():
				return
			case q.workChan <- sw.Work:
				sw.NextRun = now.Add(sw.Interval)
			default:
				// Queue is full, skip this iteration
			}
		}
	}
}

// Schedule adds a new work item to the schedule with a specified interval.
func (q *WorkQueue) Schedule(ctx context.Context, work *Work, interval time.Duration) error {
	if interval <= 0 {
		return fmt.Errorf("invalid interval: %v", interval)
	}

	key := fmt.Sprintf("%d_%s", work.Device.ID, work.Query.ParameterName)

	q.mu.Lock()
	defer q.mu.Unlock()

	// Cancel existing schedule if any
	if existing, ok := q.scheduled[key]; ok {
		close(existing.StopChan)
		delete(q.scheduled, key)
	}

	q.scheduled[key] = &ScheduledWork{
		Work:     work,
		Interval: interval,
		NextRun:  time.Now(),
		StopChan: make(chan struct{}),
	}

	return nil
}

// Cancel removes a scheduled work item based on device ID and parameter name.
func (q *WorkQueue) Cancel(deviceID int64, parameterName string) error {
	key := fmt.Sprintf("%d_%s", deviceID, parameterName)

	q.mu.Lock()
	defer q.mu.Unlock()

	if sw, ok := q.scheduled[key]; ok {
		close(sw.StopChan)
		delete(q.scheduled, key)
	}

	return nil
}

// CancelDevice cancels all scheduled work items for a specific device.
func (q *WorkQueue) CancelDevice(deviceID int64) error {
	q.mu.Lock()
	defer q.mu.Unlock()

	for key, sw := range q.scheduled {
		if sw.Work.Device.ID == deviceID {
			close(sw.StopChan)
			delete(q.scheduled, key)
		}
	}

	return nil
}

// GetScheduledWork returns a list of all scheduled work items.
func (q *WorkQueue) GetScheduledWork() []*ScheduledWork {
	q.mu.RLock()
	defer q.mu.RUnlock()

	result := make([]*ScheduledWork, 0, len(q.scheduled))
	for _, sw := range q.scheduled {
		result = append(result, sw)
	}

	return result
}

// QueueSize returns the current size of the work queue.
func (q *WorkQueue) QueueSize() int {
	return len(q.workChan)
}

// ScheduledCount returns the number of scheduled work items.
func (q *WorkQueue) ScheduledCount() int {
	q.mu.RLock()
	defer q.mu.RUnlock()
	return len(q.scheduled)
}

================
File: internal/queue/ratelimit.go
================
package queue

import (
	"context"
	"sync"
	"time"
)

// RateLimiter controls the rate of calls for different devices.
type RateLimiter struct {
	mu        sync.Mutex          // Mutex to protect access to lastCalls
	lastCalls map[int64]time.Time // deviceID -> last call time
	minWait   time.Duration       // Minimum wait time between calls
}

// NewRateLimiter creates a new RateLimiter with a specified minimum wait time.
func NewRateLimiter(minWait time.Duration) *RateLimiter {
	return &RateLimiter{
		lastCalls: make(map[int64]time.Time), // Initialize the lastCalls map
		minWait:   minWait,                   // Set the minimum wait time
	}
}

// Wait blocks until the device with the given deviceID can make a new call.
func (r *RateLimiter) Wait(ctx context.Context, deviceID int64) error {
	r.mu.Lock()                               // Lock to ensure thread-safe access to lastCalls
	lastCall, exists := r.lastCalls[deviceID] // Get the last call time for the device
	if exists {
		waitTime := r.minWait - time.Since(lastCall) // Calculate the wait time
		if waitTime > 0 {                            // If the wait time is positive, we need to wait
			r.mu.Unlock() // Unlock before waiting
			select {
			case <-ctx.Done(): // Check if the context is done
				return ctx.Err() // Return context error if done
			case <-time.After(waitTime): // Wait for the required time
			}
			r.mu.Lock() // Lock again after waiting
		}
	}
	r.lastCalls[deviceID] = time.Now() // Update the last call time for the device
	r.mu.Unlock()                      // Unlock after updating
	return nil                         // Return nil indicating success
}

================
File: internal/queue/retry.go
================
package queue

import (
	"context"
	"time"
)

type RetryPolicy struct {
	MaxRetries  int
	InitialWait time.Duration
	MaxWait     time.Duration
}

func DefaultRetryPolicy() RetryPolicy {
	return RetryPolicy{
		MaxRetries:  3,
		InitialWait: 1 * time.Second,
		MaxWait:     30 * time.Second,
	}
}

func (rp RetryPolicy) DoWithRetry(ctx context.Context, op func() error) error {
	var lastErr error
	wait := rp.InitialWait

	for attempt := 0; attempt <= rp.MaxRetries; attempt++ {
		// Execute the operation and check for errors
		if err := op(); err != nil {
			lastErr = err
			// If the maximum number of retries has been reached, exit the loop
			if attempt == rp.MaxRetries {
				break
			}

			// Wait for the context to be done or for the specified wait time
			select {
			case <-ctx.Done():
				return ctx.Err() // Return context error if done
			case <-time.After(wait):
			}

			// Exponential backoff: double the wait time for the next attempt
			wait *= 2
			if wait > rp.MaxWait {
				wait = rp.MaxWait // Cap the wait time at MaxWait
			}
			continue
		}
		return nil // Return nil if the operation was successful
	}

	return lastErr // Return the last error encountered
}
